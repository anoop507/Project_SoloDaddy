// --- Get references to HTML elements ---
const videoElement = document.querySelector('.input_video');
const canvasElement = document.querySelector('.output_canvas');
const canvasCtx = canvasElement.getContext('2d');
const displayModeElement = document.querySelector('.displayMode'); // For "MODE: PREDICT"
const gestureTextElement = document.querySelector('.gestureText'); // For "GESTURE: ..."

const enableButton = document.getElementById('enable');
const disableButton = document.getElementById('disable');
const captureButton = document.getElementById('capture');
const downloadButton = document.getElementById('download');

let model; // To store our loaded TensorFlow.js model
// IMPORTANT: Replace this with the actual path to your TensorFlow.js model files
// This should be the 'model.json' file generated by tensorflowjs_converter
const MODEL_PATH = './model.json'; 

let camera = null; // Variable to hold the MediaPipe Camera instance

// --- 1. Load your TensorFlow.js model ---
async function loadMyModel() {
    try {
        // Use tf.loadLayersModel for Keras models saved with model.save()
        // Use tf.loadGraphModel for frozen graphs or models saved with tf.saved_model.save()
        model = await tf.loadLayersModel(MODEL_PATH); 
        console.log('TensorFlow.js Model loaded successfully!');
        displayModeElement.textContent = 'MODE: PREDICT (Model Loaded)';
        gestureTextElement.textContent = 'GESTURE: Waiting for hand...';
    } catch (error) {
        console.error('Failed to load TensorFlow.js model:', error);
        displayModeElement.textContent = `MODE: ERROR (Model failed to load!)`;
        gestureTextElement.textContent = `GESTURE: Error: ${error.message}`;
        // Disable prediction related functionality if model fails to load
        // You might want to disable capture/download buttons too
        enableButton.disabled = true; 
        disableButton.disabled = true;
    }
}

// --- 2. Initialize MediaPipe Hands ---
const hands = new Hands({
    locateFile: (file) => {
        // This tells MediaPipe where to find its WASM and other model assets.
        // It's crucial for MediaPipe to work correctly.
        return `https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4/${file}`;
    }
});

hands.setOptions({
    maxNumHands: 1, // Detect up to 1 hand. Adjust to 2 if your model handles two hands.
    minDetectionConfidence: 0.7, // Minimum confidence for hand detection
    minTrackingConfidence: 0.7 // Minimum confidence for hand tracking across frames
});

// --- 3. Define MediaPipe's onResults callback ---
// This function is called every time MediaPipe processes a video frame
// and has hand landmark detection results.
hands.onResults = (results) => {
    // Clear the canvas for drawing new results
    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    
    // Draw the camera feed onto the canvas (mirrored horizontally)
    canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);

    let featureVector = []; // This will store the numerical features for your model

    // Check if hands were detected AND if the model is loaded
    if (results.multiHandLandmarks && model) {
        // Assuming your model is trained for a single hand, take the first one detected
        const handLandmarks = results.multiHandLandmarks[0];

        // --- 4. Extract and Process Landmarks into Features ---
        // This is THE CRUCIAL PART: transforming MediaPipe's output into your model's input format.
        // The most common simple approach is to flatten the x,y,z coordinates.
        // If your model was trained on angles or distances, you MUST calculate them here.
        
        for (const landmark of handLandmarks) {
            featureVector.push(landmark.x, landmark.y, landmark.z);
        }

        // IMPORTANT: Ensure your featureVector always has the exact length
        // that your TensorFlow.js model expects as input.
        // For 21 landmarks * 3 coordinates (x, y, z) = 63 features.
        const EXPECTED_FEATURE_LENGTH = 21 * 3; 

        // If no hand (or fewer than expected landmarks) detected, you might pad with zeros
        // to maintain a consistent input shape for your model.
        if (featureVector.length === 0) {
            // This scenario implies `results.multiHandLandmarks` was not null but the hand data was empty,
            // or we're explicitly handling a 'no hand' state for consistent input.
            featureVector = new Array(EXPECTED_FEATURE_LENGTH).fill(0);
        } else if (featureVector.length < EXPECTED_FEATURE_LENGTH) {
             // This case shouldn't happen if `multiHandLandmarks` is populated with 21 landmarks.
             console.warn("Incomplete landmark data. Padding with zeros.");
             while(featureVector.length < EXPECTED_FEATURE_LENGTH) {
                 featureVector.push(0);
             }
        } else if (featureVector.length > EXPECTED_FEATURE_LENGTH) {
            // If somehow more features than expected, slice it to the correct length.
            featureVector = featureVector.slice(0, EXPECTED_FEATURE_LENGTH);
        }
        
        // --- 5. Feed Features to Your TensorFlow.js Model ---
        // tf.tidy helps manage TensorFlow.js memory by cleaning up tensors after use.
        tf.tidy(() => { 
            // Create a TensorFlow.js tensor from your feature vector.
            // It needs to be a 2D tensor with shape [batch_size, num_features].
            // Here, batch_size is 1 because we're processing one frame at a time.
            const inputTensor = tf.tensor2d([featureVector], [1, featureVector.length]);
            
            // Make a prediction!
            const prediction = model.predict(inputTensor);
            
            // Get the predicted class and its confidence
            const predictedClassIndex = prediction.argMax(1).dataSync()[0];
            const confidenceScores = prediction.dataSync(); 
            const confidence = confidenceScores[predictedClassIndex] * 100; 
            
            // --- Map the predicted class number to a human-readable sign label ---
            // IMPORTANT: These labels MUST match the order of classes your model was trained on!
            const classLabels = ['Hello', 'Yes', 'No', 'Thank You', 'I Love You', 'Other/None']; 
            
            // Update the prediction display
            gestureTextElement.textContent = 
                `GESTURE: ${classLabels[predictedClassIndex]} (${confidence.toFixed(2)}%)`;
        });

        // Optional: Draw the detected hand landmarks and connections on the canvas
        mp_drawing.drawConnectors(canvasCtx, handLandmarks, mp_hands.HAND_CONNECTIONS, {color: '#00FF00', lineWidth: 5});
        mp_drawing.drawLandmarks(canvasCtx, handLandmarks, {color: '#FF0000', lineWidth: 2});

    } else {
        // If no hands detected or model not loaded yet, update display
        if (model) {
            gestureTextElement.textContent = 'GESTURE: No hand detected.';
            // You might still feed zeros to the model here if 'No hand' is a class.
            // tf.tidy(() => {
            //     const inputTensor = tf.tensor2d([new Array(EXPECTED_FEATURE_LENGTH).fill(0)], [1, EXPECTED_FEATURE_LENGTH]);
            //     const prediction = model.predict(inputTensor);
            //     // ... process 'no hand' prediction
            // });
        } else {
            gestureTextElement.textContent = 'GESTURE: Model not loaded yet.';
        }
    }
    canvasCtx.restore(); // Restore canvas state after drawing
};

// --- Webcam Control Functions ---

// Function to enable the webcam
async function enableCam() {
    if (camera) { // If camera is already running, stop it first
        camera.stop();
    }
    // Set canvas dimensions to match video (this handles mirroring too)
    // It's good to set explicit width/height on the canvas element in HTML
    canvasElement.width = videoElement.videoWidth;
    canvasElement.height = videoElement.videoHeight;


    camera = new Camera(videoElement, {
        onFrame: async () => {
            if (model) { // Only send frames if the model has finished loading
                await hands.send({ image: videoElement });
            }
        },
        width: 640,
        height: 480
    });

    try {
        await camera.start();
        console.log('Webcam started.');
        displayModeElement.textContent = 'MODE: PREDICT (Webcam ON)';
        enableButton.disabled = true;
        disableButton.disabled = false;
        captureButton.disabled = false; // Enable capture when cam is on
    } catch (error) {
        console.error('Failed to start webcam:', error);
        displayModeElement.textContent = `MODE: ERROR (Webcam failed to start!)`;
        gestureTextElement.textContent = `GESTURE: Error: ${error.message}`;
        enableButton.disabled = false;
        disableButton.disabled = true;
        captureButton.disabled = true;
    }
}

// Function to disable the webcam
function disableCam() {
    if (camera) {
        camera.stop();
        camera = null;
        console.log('Webcam stopped.');
        displayModeElement.textContent = 'MODE: IDLE';
        gestureTextElement.textContent = 'GESTURE: Webcam OFF';
        enableButton.disabled = false;
        disableButton.disabled = true;
        captureButton.disabled = true; // Disable capture when cam is off
    }
}

// --- Data Capture Functions (Conceptual) ---

// Placeholder for data capture logic
function ClickedCapture() {
    console.log("CAPTURE button clicked!");
    // You would implement logic here to:
    // 1. Get the current `featureVector` (from `hands.onResults`).
    // 2. Pair it with a label (e.g., from a dropdown or user input).
    // 3. Store it in an array or object in memory.
    // For dynamic gestures, you'd collect a sequence of featureVectors over time.
    alert("Capture functionality needs to be implemented. (Collecting data points)");
}

// Placeholder for downloading the dataset
function downloadDataset() {
    console.log("DOWNLOAD THE DATA button clicked!");
    // You would implement logic here to:
    // 1. Take the collected data from `ClickedCapture()`.
    // 2. Convert it into a downloadable format (e.g., JSON, CSV).
    // 3. Trigger a file download.
    alert("Download functionality needs to be implemented. (Saving collected data)");
}

// --- Initial Setup on Page Load ---
document.addEventListener('DOMContentLoaded', () => {
    loadMyModel(); // Load the TF.js model first

    // Initial button states
    enableButton.disabled = false;
    disableButton.disabled = true;
    captureButton.disabled = true;
    downloadButton.disabled = true; // Typically disabled until data is collected
});

// Set canvas dimensions to match video once video metadata loads
// (This handles cases where video might load before enableCam is clicked)
videoElement.addEventListener('loadedmetadata', () => {
    canvasElement.width = videoElement.videoWidth;
    canvasElement.height = videoElement.videoHeight;
});